{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0940f853-cbf7-418e-9ad6-d846c73284ee",
   "metadata": {},
   "source": [
    "1. Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c1292b-91d3-42f5-9cb5-6b49ede1fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Extract only the 'class' (sentiment) and the 'tweet' columns\n",
    "new_data = data[['class', 'tweet']].copy()\n",
    "\n",
    "# Rename the 'tweet' column as 'text'\n",
    "new_data.rename(columns={'tweet': 'text'}, inplace=True)\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)     # Remove hashtags\n",
    "    text = re.sub(r'[^a-z\\s]', '', text) # Remove non-alphabetic characters\n",
    "    text = re.sub(r'\\n+', ' ', text)     # Remove newlines\n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to the 'text' column\n",
    "new_data['clean_text'] = new_data['text'].apply(clean_text)\n",
    "print(\"Updated Data:\")\n",
    "print(new_data.head(10))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(new_data, test_size=0.33, random_state=42)\n",
    "print(f'Training Data Size: {len(train_data)}')\n",
    "print(f'Testing Data Size: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bff5dd2f-c7b9-48d8-a9e0-fdfb36d7e1f9",
   "metadata": {},
   "source": [
    "2. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8608a3c-2a5d-4d61-963a-7e57bbcf2298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: Class Distribution in the training set\n",
    "print(\"\\nClass Distribution in Training Set:\")\n",
    "print(train_data['class'].value_counts())\n",
    "\n",
    "# Custom labels for the sentiment classes\n",
    "class_labels = ['Hate Speech', 'Offensive Language', 'Normal']\n",
    "\n",
    "# Plot class distribution using a bar plot\n",
    "plt.figure(figsize=(8, 5))  # Set the figure size\n",
    "ax = sns.countplot(x='class', data=train_data, palette='viridis')  # Create the count plot with custom colors\n",
    "ax.set_title('Class Distribution in Training Set')  # Set the title of the plot\n",
    "ax.set_xlabel('Sentiment')  # Label the x-axis\n",
    "ax.set_ylabel('Count')  # Label the y-axis\n",
    "ax.set_xticks(range(len(class_labels)))  # Set the x-axis tick positions\n",
    "ax.set_xticklabels(class_labels)  # Set the x-axis tick labels\n",
    "\n",
    "# Display the bar plot\n",
    "plt.show()\n",
    "\n",
    "# Bubble plot for class distribution in the training set\n",
    "class_counts = train_data['class'].value_counts().sort_index()  # Get the counts of each class\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "colors = sns.color_palette('viridis', len(class_counts))  # Get a color palette for the bubbles\n",
    "bubble_sizes = class_counts / class_counts.max() * 10000  # Adjust the scaling factor for bubble sizes\n",
    "\n",
    "# Create the bubble plot\n",
    "for i, (label, count) in enumerate(zip(class_labels, class_counts)):\n",
    "    plt.scatter(i * 0.5, 1, s=bubble_sizes[i], c=[colors[i]], alpha=0.6, edgecolors=\"w\", linewidth=2) \n",
    "    plt.text(i * 0.5, 1, f'{label}\\n{count}', ha='center', va='center', fontsize=12, color='black')  # Annotate each bubble with the class label and count\n",
    "\n",
    "# Set the title and remove axis labels and ticks for a cleaner look\n",
    "plt.title('Bubble Plot of Class Distribution in Training Set')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.xticks([])  # Remove x-axis ticks\n",
    "plt.yticks([])  # Remove y-axis ticks\n",
    "plt.grid(False)  # Remove the grid\n",
    "\n",
    "# Adjust the x-axis limits to ensure all bubbles are visible\n",
    "plt.xlim(-0.5, 1.5)\n",
    "\n",
    "# Display the bubble plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb218d84-458a-4988-a851-1d63590cfba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. BOW Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f58ac2-32e3-4699-8f64-adba899df98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_bow = vectorizer.fit_transform(train_data['clean_text'])\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_bow = vectorizer.transform(test_data['clean_text'])\n",
    "\n",
    "# Print the shape of the resulting matrices\n",
    "print(f'Shape of X_train_bow: {X_train_bow.shape}')\n",
    "print(f'Shape of X_test_bow: {X_test_bow.shape}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "331c398d-534b-4fd6-b85a-b9a169cdcfa4",
   "metadata": {},
   "source": [
    "4. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb9352-5be4-4af1-a06c-65dee6ba957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_bow, train_data['class'])\n",
    "y_pred_log_reg = log_reg.predict(X_test_bow)\n",
    "\n",
    "# Calculate the accuracy of the Logistic Regression model and convert to percentage\n",
    "accuracy_log_reg = accuracy_score(test_data['class'], y_pred_log_reg) * 100\n",
    "print(f'Accuracy (Logistic Regression): {accuracy_log_reg:.2f}%')\n",
    "\n",
    "# Print the classification report for the Logistic Regression model\n",
    "class_report_log_reg = classification_report(test_data['class'], y_pred_log_reg, target_names=class_labels, output_dict=True)\n",
    "print('Classification Report (Logistic Regression, in %):')\n",
    "for label, metrics in class_report_log_reg.items():\n",
    "    if label in class_labels:\n",
    "        print(f'{label}:')\n",
    "        print(f\"  Precision: {metrics['precision'] * 100:.2f}%\")\n",
    "        print(f\"  Recall: {metrics['recall'] * 100:.2f}%\")\n",
    "        print(f\"  F1-score: {metrics['f1-score'] * 100:.2f}%\")\n",
    "        print(f\"  Support: {metrics['support']}\")\n",
    "\n",
    "# Compute the confusion matrix for the Logistic Regression model\n",
    "conf_matrix_log_reg = confusion_matrix(test_data['class'], y_pred_log_reg)\n",
    "\n",
    "# Normalize the confusion matrix to percentages\n",
    "conf_matrix_normalized_log_reg = conf_matrix_log_reg.astype('float') / conf_matrix_log_reg.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "# Plot the normalized confusion matrix for the Logistic Regression model\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_normalized_log_reg, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title('Confusion Matrix (Logistic Regression, in %)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "033ea22a-20f0-4b53-a502-3514b6682840",
   "metadata": {},
   "source": [
    "5. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b4937e-802c-4b6a-997c-af7d0fc2d2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm_clf = SVC(kernel='linear', C=1, max_iter=1000)\n",
    "\n",
    "# Train the model on the training data\n",
    "svm_clf.fit(X_train_bow, train_data['class'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_svm = svm_clf.predict(X_test_bow)\n",
    "\n",
    "# Calculate the accuracy of the SVM model and convert to percentage\n",
    "accuracy_svm = accuracy_score(test_data['class'], y_pred_svm) * 100\n",
    "print(f'Accuracy (SVM): {accuracy_svm:.2f}%')\n",
    "\n",
    "# Print the classification report for the SVM model\n",
    "class_report_svm = classification_report(test_data['class'], y_pred_svm, target_names=class_labels, output_dict=True)\n",
    "print('Classification Report (SVM, in %):')\n",
    "for label, metrics in class_report_svm.items():\n",
    "    if label in class_labels:\n",
    "        print(f'{label}:')\n",
    "        print(f\"  Precision: {metrics['precision'] * 100:.2f}%\")\n",
    "        print(f\"  Recall: {metrics['recall'] * 100:.2f}%\")\n",
    "        print(f\"  F1-score: {metrics['f1-score'] * 100:.2f}%\")\n",
    "        print(f\"  Support: {metrics['support']}\")\n",
    "\n",
    "# Compute the confusion matrix for the SVM model\n",
    "conf_matrix_svm = confusion_matrix(test_data['class'], y_pred_svm)\n",
    "\n",
    "# Normalize the confusion matrix to percentages\n",
    "conf_matrix_normalized_svm = conf_matrix_svm.astype('float') / conf_matrix_svm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "# Plot the normalized confusion matrix for the SVM model\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_normalized_svm, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title('Confusion Matrix (SVM, in %)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a814c0a-b9fa-4f7c-8efa-c5c2ded6ecca",
   "metadata": {},
   "source": [
    "6. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df71f74f-bb3c-4d25-b59a-3c2bb458aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the Decision Tree classifier\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "dt_clf.fit(X_train_bow, train_data['class'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_dt = dt_clf.predict(X_test_bow)\n",
    "\n",
    "# Calculate the accuracy of the Decision Tree model and convert to percentage\n",
    "accuracy_dt = accuracy_score(test_data['class'], y_pred_dt) * 100\n",
    "print(f'Accuracy (Decision Tree): {accuracy_dt:.2f}%')\n",
    "\n",
    "# Print the classification report for the Decision Tree model\n",
    "class_report_dt = classification_report(test_data['class'], y_pred_dt, target_names=class_labels, output_dict=True)\n",
    "print('Classification Report (Decision Tree, in %):')\n",
    "for label, metrics in class_report_dt.items():\n",
    "    if label in class_labels:\n",
    "        print(f'{label}:')\n",
    "        print(f\"  Precision: {metrics['precision'] * 100:.2f}%\")\n",
    "        print(f\"  Recall: {metrics['recall'] * 100:.2f}%\")\n",
    "        print(f\"  F1-score: {metrics['f1-score'] * 100:.2f}%\")\n",
    "        print(f\"  Support: {metrics['support']}\")\n",
    "\n",
    "# Compute the confusion matrix for the Decision Tree model\n",
    "conf_matrix_dt = confusion_matrix(test_data['class'], y_pred_dt)\n",
    "\n",
    "# Normalize the confusion matrix to percentages\n",
    "conf_matrix_normalized_dt = conf_matrix_dt.astype('float') / conf_matrix_dt.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "# Plot the normalized confusion matrix for the Decision Tree model\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_normalized_dt, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title('Confusion Matrix (Decision Tree, in %)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "173f4c7e-1581-4104-933e-ea2eea358fe8",
   "metadata": {},
   "source": [
    "7. Comparing model accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8196610-3e1a-43ba-ae13-d14aac29155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the accuracies of the Logistic Regression, SVM, and Decision Tree models\n",
    "print(f'Logistic Regression Accuracy: {accuracy_log_reg:.2f}%')\n",
    "print(f'SVM Accuracy: {accuracy_svm:.2f}%')\n",
    "print(f'Decision Tree Accuracy: {accuracy_dt:.2f}%')\n",
    "\n",
    "# Store the results in a dictionary\n",
    "accuracy_results = {\n",
    "    'Logistic Regression': accuracy_log_reg,\n",
    "    'Support Vector Machine (SVM)': accuracy_svm,\n",
    "    'Decision Tree': accuracy_dt,\n",
    "}\n",
    "\n",
    "# Print the accuracy results\n",
    "print('Accuracy Comparison:')\n",
    "for model, accuracy in accuracy_results.items():\n",
    "    print(f'{model}: {accuracy:.2f}%')\n",
    "\n",
    "# Plot the accuracies for comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(accuracy_results.keys(), accuracy_results.values(), color=['blue', 'green', 'orange'])\n",
    "plt.title('Comparison of Model Accuracies')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim(0, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f90a729-805e-44b4-a4fa-d1307244e7ee",
   "metadata": {},
   "source": [
    "8. Hyperparameter Tuning for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78659d7d-b6cc-4a05-b94a-e821b0a0afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the parameter grid for Logistic Regression\n",
    "# 'C' is the regularization parameter. Lower values specify stronger regularization.\n",
    "# 'solver' is the algorithm to use in the optimization problem.\n",
    "# 'max_iter' is the maximum number of iterations taken for the solvers to converge.\n",
    "param_grid_log_reg = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'lbfgs'],  # Limiting solvers to avoid convergence issues\n",
    "    'max_iter': [1000, 2000, 3000, 5000]  # Increased max_iter values to ensure convergence\n",
    "}\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Initialize GridSearchCV with cross-validation\n",
    "# GridSearchCV is a method for performing hyperparameter tuning.\n",
    "# It exhaustively searches over a specified parameter grid to find the best parameters.\n",
    "# cv=5 means 5-fold cross-validation, scoring='accuracy' means we evaluate the models based on accuracy.\n",
    "# verbose=2 gives detailed logs of the search process, n_jobs=-1 uses all available cores for computation.\n",
    "grid_search_log_reg = GridSearchCV(estimator=log_reg, param_grid=param_grid_log_reg, cv=5, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "# This step performs the exhaustive search over the parameter grid and fits the models.\n",
    "grid_search_log_reg.fit(X_train_bow, train_data['class'])\n",
    "\n",
    "# Print the best parameters and the best score found by GridSearchCV\n",
    "print(f'Best Parameters: {grid_search_log_reg.best_params_}')\n",
    "print(f'Best Cross-Validation Score: {grid_search_log_reg.best_score_}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a14f9ff-ab80-4dda-982b-5d03e4418da4",
   "metadata": {},
   "source": [
    "9. Training and Evaluating Updated Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b7bbaf-5ae6-4027-8c6e-fbba21dcaacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Train the updated Logistic Regression model with the best parameters\n",
    "best_log_reg = grid_search_log_reg.best_estimator_\n",
    "best_log_reg.fit(X_train_bow, train_data['class'])\n",
    "y_pred_updated = best_log_reg.predict(X_test_bow)\n",
    "\n",
    "# Calculate the accuracy of the updated Logistic Regression model and convert to percentage\n",
    "accuracy_updated = accuracy_score(test_data['class'], y_pred_updated) * 100\n",
    "print(f'Accuracy (Updated Logistic Regression): {accuracy_updated:.2f}%')\n",
    "\n",
    "# Print the classification report for the updated Logistic Regression model\n",
    "class_report_updated = classification_report(test_data['class'], y_pred_updated, target_names=class_labels, output_dict=True)\n",
    "print('Classification Report (Updated Logistic Regression, in %):')\n",
    "for label, metrics in class_report_updated.items():\n",
    "    if label in class_labels:\n",
    "        print(f'{label}:')\n",
    "        print(f\"  Precision: {metrics['precision'] * 100:.2f}%\")\n",
    "        print(f\"  Recall: {metrics['recall'] * 100:.2f}%\")\n",
    "        print(f\"  F1-score: {metrics['f1-score'] * 100:.2f}%\")\n",
    "        print(f\"  Support: {metrics['support']}\")\n",
    "\n",
    "# Compute the confusion matrix for the updated Logistic Regression model\n",
    "conf_matrix_updated = confusion_matrix(test_data['class'], y_pred_updated)\n",
    "\n",
    "# Normalize the confusion matrix to percentages\n",
    "conf_matrix_normalized_updated = conf_matrix_updated.astype('float') / conf_matrix_updated.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "# Plot the normalized confusion matrix for the updated Logistic Regression model\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_normalized_updated, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title('Confusion Matrix (Updated Logistic Regression, in %)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d016e00-16b6-429d-aba5-567e24948325",
   "metadata": {},
   "source": [
    "10. Comparing Original and Updated Logistic Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727b8d76-8701-4d06-9d3a-ff65e9581780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy for each model\n",
    "accuracy_log_reg_original = accuracy_score(test_data['class'], y_pred_log_reg) * 100\n",
    "accuracy_log_reg_updated = accuracy_score(test_data['class'], y_pred_updated) * 100\n",
    "\n",
    "# Store the results in a dictionary\n",
    "accuracy_results = {\n",
    "    'Logistic Regression (Original)': accuracy_log_reg_original,\n",
    "    'Logistic Regression (with t)': accuracy_log_reg_updated,\n",
    "    'Support Vector Machine (SVM)': accuracy_svm,\n",
    "    'Decision Tree': accuracy_dt\n",
    "}\n",
    "\n",
    "# Print the accuracy results\n",
    "print('Accuracy Comparison:')\n",
    "for model, accuracy in accuracy_results.items():\n",
    "    print(f'{model}: {accuracy:.2f}%')\n",
    "\n",
    "# Plot the accuracies for comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(accuracy_results.keys(), accuracy_results.values(), color=['blue', 'green', 'orange', 'red'])\n",
    "plt.title('Comparison of Model Accuracies')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim(0, 100)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5de2f9d-c16c-4f21-b874-6d476b16373c",
   "metadata": {},
   "source": [
    "11. Model in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daed0ff3-30a1-4f20-997c-cea339366814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class_labels = ['Hate Speech', 'Offensive Language', 'Normal']\n",
    "\n",
    "# Function to predict sentiment of input text\n",
    "def predict_sentiment(input_text):\n",
    "    # Clean the input text using function in step 1\n",
    "    clean_input = clean_text(input_text)\n",
    "    \n",
    "    # Transform the cleaned input text to bag-of-words representation\n",
    "    input_vector = vectorizer.transform([clean_input])\n",
    "    \n",
    "    # Predict the sentiment class using the trained regression model\n",
    "    predicted_class = best_log_reg.predict(input_vector)\n",
    "    \n",
    "    # Get the sentiment label\n",
    "    sentiment_label = class_labels[predicted_class[0]]\n",
    "    \n",
    "    return sentiment_label\n",
    "\n",
    "# Example usage\n",
    "input_text = input(\"Enter a tweet to predict its sentiment: \")\n",
    "predicted_sentiment = predict_sentiment(input_text)\n",
    "print(f'Predicted Sentiment: {predicted_sentiment}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98fd2ab-c95e-4079-9d57-a337eb8dbfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Assuming 'train_data' and 'test_data' are already defined\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = vectorizer.fit_transform(train_data['clean_text'])\n",
    "X_test_bow = vectorizer.transform(test_data['clean_text'])\n",
    "\n",
    "# Train Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_bow, train_data['class'])\n",
    "\n",
    "# Save the trained model and the vectorizer\n",
    "joblib.dump(log_reg, 'best_log_reg.pkl')\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
